{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from psycopg2 import connect\n",
    "import pandas as pd\n",
    "import pm4py\n",
    "import numpy as np\n",
    "import pandasql as ps\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from scipy.stats import variation\n",
    "from scipy import stats\n",
    "from pm4py.algo.discovery.dfg import algorithm as dfg_discovery\n",
    "from pm4py.visualization.dfg import visualizer as dfg_visualization\n",
    "from pm4py.statistics.eventually_follows.log import get as efg_get\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing as sk\n",
    "from scipy.stats import chi2_contingency\n",
    "import math\n",
    "import statistics\n",
    "import pingouin as pg\n",
    "import graphviz\n",
    "from statsmodels.stats import multitest\n",
    "from statsmodels.stats.contingency_tables import SquareTable as ST\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MIMIC-IV\n",
    "final_pm = pd.read_csv(\"Outputs/MIMIC_Transformed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sepsis\n",
    "final_pm = pd.read_csv(\"Outputs/Sepsis_Transformed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadms = list(final_pm[\"case:hadm_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case:hadm_id'}\n",
    "event_log = pm4py.format_dataframe(final_pm, case_id='case:hadm_id', activity_key='concept:name', timestamp_key='time:timestamp')\n",
    "log = pm4py.convert_to_event_log(event_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve all possible process variants and remove variants occuring < 20 times due to their small sample size\n",
    "from pm4py.algo.filtering.log.variants import variants_filter\n",
    "variants = variants_filter.get_variants(log)\n",
    "variants = list(variants.keys())\n",
    "var = final_pm.groupby('case:hadm_id')['concept:name'].apply(list).reset_index()\n",
    "var[\"concept:name\"] = var['concept:name'].apply(lambda x: ','.join(map(str, x)))\n",
    "var = var.rename({\"concept:name\":\"variant\"}, axis=1)\n",
    "final_pm_var = final_pm.merge(var, how=\"left\", on=\"case:hadm_id\")\n",
    "var_count= final_pm_var.drop_duplicates(\"case:hadm_id\").groupby(\"variant\").count()\n",
    "to_drop = list(var_count.loc[var_count[\"case:hadm_id\"] < 20].reset_index()[\"variant\"])\n",
    "for ele in to_drop:\n",
    "    variants.remove(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_attributes(proc_c):\n",
    "    for index, row in proc_c.iterrows():\n",
    "        if((row[\"numberOfActivities\"] == 1) & (row[\"numberOfTraceOccurence (Mean)\"] == 1)):\n",
    "            proc_c.at[index, \"class\"] = \"static\"\n",
    "        elif((row[\"numberOfActivities\"] > 1) & (row[\"numberOfTraceOccurence (Mean)\"] == 1)):\n",
    "            proc_c.at[index, \"class\"] = \"semi-dynamic\"\n",
    "        else:\n",
    "            proc_c.at[index, \"class\"] = \"dynamic\"\n",
    "    return proc_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify activity column\n",
    "activity = \"concept:name\"\n",
    "#specify case id\n",
    "case_id = \"case:hadm_id\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify attributes which should not be classified\n",
    "#MIMIC\n",
    "columns_to_drop = ['Unnamed: 0','ordercategoryname','category','time:timestamp', \"Unnamed: 0.1\", \"event_time\", \"time_diff\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify attributes which should not be classified\n",
    "#Sepsis\n",
    "columns_to_drop = ['Unnamed: 0','time:timestamp', \"event_time\", \"time_diff\", \"Variant\", \"Variant index\", \"lifecycle:transition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Classify event attributes, so that dynamic event attributes can be identified\n",
    "final_pm_class = final_pm.drop(columns_to_drop, axis=1)\n",
    "\n",
    "activities = final_pm_class[activity].unique()\n",
    "\n",
    "matrix = pd.DataFrame(data=None, columns=activities)\n",
    "\n",
    "#identify attributes for activities\n",
    "att_card = pd.DataFrame(data=None,columns=final_pm_class.columns)\n",
    "for dep in activities:\n",
    "    dep_data = final_pm_class.loc[final_pm_class[activity] == dep]\n",
    "    y = dep_data.groupby(activity).agg({lambda x: x.notnull().sum()})\n",
    "    y.columns = y.columns.droplevel(1)\n",
    "    y = y.reset_index().drop(activity, axis=1)\n",
    "    row_num = len(dep_data)\n",
    "    row = y.loc[0]\n",
    "    for col in y.columns:\n",
    "        t = 0.05\n",
    "        if(row[col] > (row_num*t)):\n",
    "            row[col] = 1\n",
    "        else:\n",
    "            row[col] = 0\n",
    "    row[activity] = dep\n",
    "    att_card = att_card.append(row)\n",
    "    \n",
    "\n",
    "\n",
    "att_card.drop(case_id, axis=1, inplace=True)\n",
    "\n",
    "# for each attribute: number of activities + number of occurence in a trace\n",
    "\n",
    "number_trace_occurence = final_pm_class.groupby(case_id).agg({lambda x: x.notnull().sum()})\n",
    "\n",
    "#drop concept:name\n",
    "number_trace_occurence.drop(activity, axis=1, inplace=True)\n",
    "\n",
    "number_trace_occurence.columns = number_trace_occurence.columns.droplevel(1)\n",
    "\n",
    "number_trace_occurence = number_trace_occurence.replace(0, np.NaN)\n",
    "\n",
    "number_trace_occurence = number_trace_occurence.mean()\n",
    "\n",
    "number_trace_occurence = number_trace_occurence.rename(\"numberOfTraceOccurence (Mean)\")\n",
    "\n",
    "number_of_activities = pd.Series([], name=\"numberOfActivities\")\n",
    "\n",
    "for col in final_pm_class.columns:\n",
    "    if((col != case_id) & (col != activity)):\n",
    "        number_of_activities[col] = len(final_pm_class[[activity, col]].dropna()[activity].unique())\n",
    "\n",
    "process_characteristics = pd.concat([number_of_activities, number_trace_occurence], axis=1)\n",
    "\n",
    "for col in final_pm_class.columns:\n",
    "    if (final_pm_class[col].nunique()/final_pm_class[col].count() < 0.005):\n",
    "        process_characteristics.loc[col, \"type\"] = \"categorical\"\n",
    "    else:\n",
    "        process_characteristics.loc[col, \"type\"] = \"continuous\"\n",
    "\n",
    "process_characteristics = process_characteristics.drop(labels=[case_id, activity])\n",
    "\n",
    "x = process_characteristics\n",
    "\n",
    "x = classify_attributes(process_characteristics)\n",
    "\n",
    "x = x.reset_index()\n",
    "\n",
    "x = x.rename({\"index\":\"Activity\"}, axis=1)\n",
    "\n",
    "attribute_classes = x[[\"Activity\", \"class\", \"type\"]]\n",
    "\n",
    "attribute_classes[\"CV\"] = 0\n",
    "\n",
    "attribute_list_con = list(attribute_classes.loc[(attribute_classes[\"class\"] == \"dynamic\") & (attribute_classes[\"type\"] == \"continuous\")][\"Activity\"])\n",
    "\n",
    "attribute_list_cat = list(attribute_classes.loc[(attribute_classes[\"class\"] == \"dynamic\") & (attribute_classes[\"type\"] == \"categorical\")][\"Activity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = dfg_discovery.apply(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove small sample size relations (optional)\n",
    "l = list()\n",
    "for x in dfg:\n",
    "    if (dfg[x] <= 30):\n",
    "        l.append(x)\n",
    "for e in l:\n",
    "    del(dfg[e])       \n",
    "\n",
    "efg_graph = efg_get.apply(log)\n",
    "\n",
    "#remove small sample size relations (optional)\n",
    "l = list()\n",
    "for x in efg_graph:\n",
    "    if (efg_graph[x] <= 30):\n",
    "        l.append(x)\n",
    "for e in l:\n",
    "    del(efg_graph[e])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consecutive_hadms(df, act_1, act_2):\n",
    "    df = df.loc[df[activity].isin([act_1, act_2])]\n",
    "    l = [] \n",
    "    hadms = df[case_id].unique()\n",
    "    rows_list = []\n",
    "    for hadm_id in hadms:\n",
    "        curr_act = \"\"\n",
    "        index_1 = 0\n",
    "        first_row = \"\"\n",
    "        df_hadm = df.loc[df[case_id] == hadm_id]\n",
    "        for index, row in df_hadm.iterrows():\n",
    "        #first act\n",
    "            if((row[activity] == act_1) & (curr_act == \"\")):\n",
    "                curr_act = row[activity]\n",
    "                index_1 = index\n",
    "                first_row = row\n",
    "                continue\n",
    "            elif((curr_act != \"\") & (row[activity] == act_2)):\n",
    "                if(index - index_1 == 1):\n",
    "                    rows_list.append(first_row)\n",
    "                    rows_list.append(row)\n",
    "                    curr_act = \"\"\n",
    "                else:\n",
    "                    curr_act = \"\"\n",
    "                    \n",
    "    return pd.DataFrame(rows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eventually_follow_hadms(df, act_1, act_2):\n",
    "    df = df.loc[df[activity].isin([act_1, act_2])]\n",
    "    l = [] \n",
    "    hadms = df[case_id].unique()\n",
    "    rows_list = []\n",
    "    for hadm_id in hadms:\n",
    "        curr_act = \"\"\n",
    "        first_row = \"\"\n",
    "        df_hadm = df.loc[df[case_id] == hadm_id]\n",
    "        for index, row in df_hadm.iterrows():\n",
    "        #first act\n",
    "            if((row[activity] == act_1) & (curr_act == \"\")):\n",
    "                curr_act = row[activity]\n",
    "                first_row = row\n",
    "            elif((curr_act != \"\") & (row[activity] == act_2)):\n",
    "                rows_list.append(first_row)\n",
    "                rows_list.append(row)\n",
    "                curr_act = \"\"\n",
    "                \n",
    "    return pd.DataFrame(rows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_value_con(dep_1, dep_2, ea, df):\n",
    "    f1 = df.loc[df[activity] == dep_1][ea].to_frame().reset_index().drop(\"index\", axis=1)\n",
    "    f2 = df.loc[df[activity] == dep_2][ea].to_frame().reset_index().drop(\"index\", axis=1)\n",
    "    df_wo_na = pd.concat([f1,f2], axis= 1)\n",
    "    df_wo_na.columns = pd.RangeIndex(df_wo_na.columns.size)\n",
    "    df_wo_na = df_wo_na.dropna()\n",
    "    \n",
    "    l1 = list(df_wo_na[0])\n",
    "    l2 = list(df_wo_na[1])\n",
    "    df1 = df_wo_na[0]\n",
    "    df2 = df_wo_na[1]\n",
    "    \n",
    "    if((len(l1) < 8) | (len(l2) < 8)):\n",
    "        return(np.nan,np.nan, np.nan, np.nan,np.nan,np.nan, np.nan, np.nan)\n",
    "    try:\n",
    "        p = pg.wilcoxon(l1, l2)[\"p-val\"][0]\n",
    "        cles = pg.wilcoxon(l1, l2)[\"CLES\"][0]\n",
    "        rbc = pg.wilcoxon(l1, l2)[\"RBC\"][0]\n",
    "        z = stats.norm.isf(p / 2)\n",
    "        r = z / np.sqrt(len(l1)*2)        \n",
    "        cohen = 2*r / np.sqrt(1-np.square(r))\n",
    "        return (p, cles, rbc, len(l1), df1.mean(), df2.mean(), df1.std(), df2.std())\n",
    "    except:\n",
    "        return(1,0,0,0, 0, 0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "con_All = pd.DataFrame()\n",
    "df_con = pd.DataFrame()\n",
    "for rel in dfg:\n",
    "    #varianten aus consecutive df extrahieren\n",
    "    consecutive_df = consecutive_hadms(final_pm_var, rel[0], rel[1])\n",
    "    variants = consecutive_df[\"variant\"].unique()\n",
    "    att_list = att_card.loc[att_card[activity].isin([rel[0], rel[1]])].sum().to_frame().reset_index()\n",
    "    att_list = att_list.rename({\"index\":\"e_At\", 0:\"cardinality\"}, axis=1)\n",
    "    att_list = att_list.loc[(att_list[\"cardinality\"] == 2) & (att_list[\"e_At\"].isin(attribute_list_con))].reset_index()\n",
    "    for e_at in att_list[\"e_At\"]:\n",
    "        p, cles, rbc, num_p, m1, m2, st1, st2 = stat_value_con(rel[0], rel[1], e_at, consecutive_df)\n",
    "        con_All = con_All.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"RBC\": rbc, 'abs(RBC)': abs(rbc), 'var' : 'ALL', '#Patients' : num_p, 'M1':m1, 'M2':m2, 'ST1':st1, 'ST2':st2, 'Directly':True}, ignore_index=True)    \n",
    "        if(p <= (0.05 / len(att_list))):\n",
    "            df_con = df_con.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"RBC\": rbc, 'abs(RBC)': abs(rbc), 'var' : 'ALL', '#Patients' : num_p, 'M1':m1, 'M2':m2, 'ST1':st1, 'ST2':st2, 'Directly':True}, ignore_index=True)\n",
    "        for var in variants:\n",
    "            df_var = consecutive_df.loc[consecutive_df[\"variant\"] == var]\n",
    "            p, cles, rbc, num_p, m1, m2, st1, st2 = stat_value_con(rel[0], rel[1], e_at, df_var)\n",
    "            con_All = con_All.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"RBC\": rbc, 'abs(RBC)': abs(rbc), 'var' : var, '#Patients' : num_p, 'M1':m1, 'M2':m2, 'ST1':st1, 'ST2':st2, 'Directly':True}, ignore_index=True)\n",
    "            if(p <= (0.05 / len(att_list))):\n",
    "                df_con = df_con.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"RBC\": rbc, 'abs(RBC)': abs(rbc), 'var' : var, '#Patients' : num_p, 'M1':m1, 'M2':m2, 'ST1':st1, 'ST2':st2, 'Directly':True}, ignore_index=True)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rel in efg_graph:\n",
    "    #varianten aus consecutive df extrahieren\n",
    "    consecutive_df = eventually_follow_hadms(final_pm_var, rel[0], rel[1])\n",
    "    variants = consecutive_df[\"variant\"].unique()\n",
    "    att_list = att_card.loc[att_card[activity].isin([rel[0], rel[1]])].sum().to_frame().reset_index()\n",
    "    att_list = att_list.rename({\"index\":\"e_At\", 0:\"cardinality\"}, axis=1)\n",
    "    att_list = att_list.loc[(att_list[\"cardinality\"] == 2) & (att_list[\"e_At\"].isin(attribute_list_con))].reset_index()\n",
    "    for e_at in att_list[\"e_At\"]:\n",
    "        p, cles, rbc, num_p, m1, m2, st1, st2 = stat_value_con(rel[0], rel[1], e_at, consecutive_df)\n",
    "        con_All = con_All.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"RBC\": rbc, 'abs(RBC)': abs(rbc), 'var' : 'ALL', '#Patients' : num_p, 'M1':m1, 'M2':m2, 'ST1':st1, 'ST2':st2, 'Directly':False}, ignore_index=True)    \n",
    "        if(p <= (0.05 / len(att_list))):\n",
    "            df_con = df_con.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"RBC\": rbc, 'abs(RBC)': abs(rbc), 'var' : 'ALL', '#Patients' : num_p, 'M1':m1, 'M2':m2, 'ST1':st1, 'ST2':st2, 'Directly':False}, ignore_index=True)\n",
    "            for var in variants:\n",
    "                df_var = consecutive_df.loc[consecutive_df[\"variant\"] == var]\n",
    "                p, cles, rbc, num_p, m1, m2, st1, st2 = stat_value_con(rel[0], rel[1], e_at, df_var)\n",
    "                con_All = con_All.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"RBC\": rbc, 'abs(RBC)': abs(rbc), 'var' : var, '#Patients' : num_p, 'M1':m1, 'M2':m2, 'ST1':st1, 'ST2':st2, 'Directly':False}, ignore_index=True)\n",
    "                if(p <= (0.05 / len(att_list))):\n",
    "                    df_con = df_con.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"RBC\": rbc, 'abs(RBC)': abs(rbc), 'var' : var, '#Patients' : num_p, 'M1':m1, 'M2':m2, 'ST1':st1, 'ST2':st2, 'Directly':False}, ignore_index=True)\n",
    "\n",
    "con_All = con_All.loc[~con_All[\"P\"].isna()]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Sepsis\n",
    "con_All.to_csv(\"Outputs/Change_Detection_Sepsis_con_All.csv\")\n",
    "df_con.to_csv(\"Outputs/Change_Detection_Sepsis_df_con.csv\")\n",
    "final_pm_var.to_csv(\"Outputs/Sepsis_Transformed_Var.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MIMIC-IV\n",
    "df_con = df_con.drop_duplicates([\"Act_1\", \"Act_2\", \"E_At\", \"var\"])\n",
    "con_All = con_All.drop_duplicates([\"Act_1\", \"Act_2\", \"E_At\", \"var\"])\n",
    "con_All.to_csv(\"Outputs/Change_Detection_MIMIC_con_All.csv\")\n",
    "df_con.to_csv(\"Outputs/Change_Detection_MIMIC_df_con.csv\")\n",
    "final_pm_var.to_csv(\"Outputs/MIMIC_Transformed_Var.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
