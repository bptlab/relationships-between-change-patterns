{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\anaconda3\\lib\\site-packages\\outdated\\utils.py:14: OutdatedPackageWarning: The package pingouin is out of date. Your version is 0.4.0, the latest is 0.5.3.\n",
      "Set the environment variable OUTDATED_IGNORE=1 to disable these warnings.\n",
      "  return warn(\n",
      "C:\\Users\\jonas\\anaconda3\\lib\\site-packages\\outdated\\utils.py:14: OutdatedPackageWarning: The package outdated is out of date. Your version is 0.2.1, the latest is 0.2.2.\n",
      "Set the environment variable OUTDATED_IGNORE=1 to disable these warnings.\n",
      "  return warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from psycopg2 import connect\n",
    "import pandas as pd\n",
    "import pm4py\n",
    "import numpy as np\n",
    "import pandasql as ps\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from scipy.stats import variation\n",
    "from scipy import stats\n",
    "from pm4py.algo.discovery.dfg import algorithm as dfg_discovery\n",
    "from pm4py.visualization.dfg import visualizer as dfg_visualization\n",
    "from pm4py.statistics.eventually_follows.log import get as efg_get\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing as sk\n",
    "from scipy.stats import chi2_contingency\n",
    "import math\n",
    "import statistics\n",
    "import pingouin as pg\n",
    "import graphviz\n",
    "from statsmodels.stats import multitest\n",
    "from statsmodels.stats.contingency_tables import SquareTable as ST\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sepsis\n",
    "final_pm = pd.read_csv(\"Logs/Kidney_Failure_Log.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pm.rename(columns={\"hadm_id\":\"case:hadm_id\", \"department\":\"concept:name\", \"intime\":\"time:timestamp\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadms = list(final_pm[\"case:hadm_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case:hadm_id'}\n",
    "event_log = pm4py.format_dataframe(final_pm, case_id='case:hadm_id', activity_key='concept:name', timestamp_key='time:timestamp')\n",
    "log = pm4py.convert_to_event_log(event_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve all possible process variants and remove variants occuring < 20 times due to their small sample size\n",
    "from pm4py.algo.filtering.log.variants import variants_filter\n",
    "variants = variants_filter.get_variants(log)\n",
    "variants = list(variants.keys())\n",
    "var = final_pm.groupby('case:hadm_id')['concept:name'].apply(list).reset_index()\n",
    "var[\"concept:name\"] = var['concept:name'].apply(lambda x: ','.join(map(str, x)))\n",
    "var = var.rename({\"concept:name\":\"variant\"}, axis=1)\n",
    "final_pm_var = final_pm.merge(var, how=\"left\", on=\"case:hadm_id\")\n",
    "var_count= final_pm_var.drop_duplicates(\"case:hadm_id\").groupby(\"variant\").count()\n",
    "to_drop = list(var_count.loc[var_count[\"case:hadm_id\"] < 20].reset_index()[\"variant\"])\n",
    "for ele in to_drop:\n",
    "    variants.remove(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_attributes(proc_c):\n",
    "    for index, row in proc_c.iterrows():\n",
    "        if((row[\"numberOfActivities\"] == 1) & (row[\"numberOfTraceOccurence (Mean)\"] == 1)):\n",
    "            proc_c.at[index, \"class\"] = \"static\"\n",
    "        elif((row[\"numberOfActivities\"] > 1) & (row[\"numberOfTraceOccurence (Mean)\"] == 1)):\n",
    "            proc_c.at[index, \"class\"] = \"semi-dynamic\"\n",
    "        else:\n",
    "            proc_c.at[index, \"class\"] = \"dynamic\"\n",
    "    return proc_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify activity column\n",
    "activity = \"concept:name\"\n",
    "#specify case id\n",
    "case_id = \"case:hadm_id\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify attributes which should not be classified\n",
    "#MIMIC\n",
    "columns_to_drop = ['Unnamed: 0', 'outtime', 'time:timestamp', 'subject_id', 'transfer_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-db404fd8a9bb>:45: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  number_of_activities = pd.Series([], name=\"numberOfActivities\")\n",
      "<ipython-input-40-db404fd8a9bb>:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  attribute_classes[\"CV\"] = 0\n"
     ]
    }
   ],
   "source": [
    "#Classify event attributes, so that dynamic event attributes can be identified\n",
    "final_pm_class = final_pm.drop(columns_to_drop, axis=1)\n",
    "\n",
    "activities = final_pm_class[activity].unique()\n",
    "\n",
    "matrix = pd.DataFrame(data=None, columns=activities)\n",
    "\n",
    "#identify attributes for activities\n",
    "att_card = pd.DataFrame(data=None,columns=final_pm_class.columns)\n",
    "for dep in activities:\n",
    "    dep_data = final_pm_class.loc[final_pm_class[activity] == dep]\n",
    "    y = dep_data.groupby(activity).agg({lambda x: x.notnull().sum()})\n",
    "    y.columns = y.columns.droplevel(1)\n",
    "    y = y.reset_index().drop(activity, axis=1)\n",
    "    row_num = len(dep_data)\n",
    "    row = y.loc[0]\n",
    "    for col in y.columns:\n",
    "        t = 0.05\n",
    "        if(row[col] > (row_num*t)):\n",
    "            row[col] = 1\n",
    "        else:\n",
    "            row[col] = 0\n",
    "    row[activity] = dep\n",
    "    att_card = att_card.append(row)\n",
    "    \n",
    "\n",
    "\n",
    "att_card.drop(case_id, axis=1, inplace=True)\n",
    "\n",
    "# for each attribute: number of activities + number of occurence in a trace\n",
    "\n",
    "number_trace_occurence = final_pm_class.groupby(case_id).agg({lambda x: x.notnull().sum()})\n",
    "\n",
    "#drop concept:name\n",
    "number_trace_occurence.drop(activity, axis=1, inplace=True)\n",
    "\n",
    "number_trace_occurence.columns = number_trace_occurence.columns.droplevel(1)\n",
    "\n",
    "number_trace_occurence = number_trace_occurence.replace(0, np.NaN)\n",
    "\n",
    "number_trace_occurence = number_trace_occurence.mean()\n",
    "\n",
    "number_trace_occurence = number_trace_occurence.rename(\"numberOfTraceOccurence (Mean)\")\n",
    "\n",
    "number_of_activities = pd.Series([], name=\"numberOfActivities\")\n",
    "\n",
    "for col in final_pm_class.columns:\n",
    "    if((col != case_id) & (col != activity)):\n",
    "        number_of_activities[col] = len(final_pm_class[[activity, col]].dropna()[activity].unique())\n",
    "\n",
    "process_characteristics = pd.concat([number_of_activities, number_trace_occurence], axis=1)\n",
    "\n",
    "for col in final_pm_class.columns:\n",
    "    if (final_pm_class[col].nunique()/final_pm_class[col].count() < 0.05):\n",
    "        process_characteristics.loc[col, \"type\"] = \"categorical\"\n",
    "    else:\n",
    "        process_characteristics.loc[col, \"type\"] = \"continuous\"\n",
    "\n",
    "process_characteristics = process_characteristics.drop(labels=[case_id, activity])\n",
    "\n",
    "x = process_characteristics\n",
    "\n",
    "x = classify_attributes(process_characteristics)\n",
    "\n",
    "x = x.reset_index()\n",
    "\n",
    "x = x.rename({\"index\":\"Activity\"}, axis=1)\n",
    "\n",
    "attribute_classes = x[[\"Activity\", \"class\", \"type\"]]\n",
    "\n",
    "attribute_classes[\"CV\"] = 0\n",
    "\n",
    "attribute_list_con = list(attribute_classes.loc[(attribute_classes[\"class\"] == \"dynamic\") & (attribute_classes[\"type\"] == \"continuous\")][\"Activity\"])\n",
    "\n",
    "attribute_list_cat = list(attribute_classes.loc[(attribute_classes[\"class\"] == \"dynamic\") & (attribute_classes[\"type\"] == \"categorical\")][\"Activity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = dfg_discovery.apply(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove small sample size relations (optional)\n",
    "l = list()\n",
    "for x in dfg:\n",
    "    if (dfg[x] <= 30):\n",
    "        l.append(x)\n",
    "for e in l:\n",
    "    del(dfg[e])       \n",
    "\n",
    "efg_graph = efg_get.apply(log)\n",
    "\n",
    "#remove small sample size relations (optional)\n",
    "l = list()\n",
    "for x in efg_graph:\n",
    "    if (efg_graph[x] <= 30):\n",
    "        l.append(x)\n",
    "for e in l:\n",
    "    del(efg_graph[e])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consecutive_hadms(df, act_1, act_2):\n",
    "    df = df.loc[df[activity].isin([act_1, act_2])]\n",
    "    l = [] \n",
    "    hadms = df[case_id].unique()\n",
    "    rows_list = []\n",
    "    for hadm_id in hadms:\n",
    "        curr_act = \"\"\n",
    "        index_1 = 0\n",
    "        first_row = \"\"\n",
    "        df_hadm = df.loc[df[case_id] == hadm_id]\n",
    "        for index, row in df_hadm.iterrows():\n",
    "        #first act\n",
    "            if((row[activity] == act_1) & (curr_act == \"\")):\n",
    "                curr_act = row[activity]\n",
    "                index_1 = index\n",
    "                first_row = row\n",
    "                continue\n",
    "            elif((curr_act != \"\") & (row[activity] == act_2)):\n",
    "                if(index - index_1 == 1):\n",
    "                    rows_list.append(first_row)\n",
    "                    rows_list.append(row)\n",
    "                    curr_act = \"\"\n",
    "                else:\n",
    "                    curr_act = \"\"\n",
    "                    \n",
    "    return pd.DataFrame(rows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eventually_follow_hadms(df, act_1, act_2):\n",
    "    df = df.loc[df[activity].isin([act_1, act_2])]\n",
    "    l = [] \n",
    "    hadms = df[case_id].unique()\n",
    "    rows_list = []\n",
    "    for hadm_id in hadms:\n",
    "        curr_act = \"\"\n",
    "        first_row = \"\"\n",
    "        df_hadm = df.loc[df[case_id] == hadm_id]\n",
    "        for index, row in df_hadm.iterrows():\n",
    "        #first act\n",
    "            if((row[activity] == act_1) & (curr_act == \"\")):\n",
    "                curr_act = row[activity]\n",
    "                first_row = row\n",
    "            elif((curr_act != \"\") & (row[activity] == act_2)):\n",
    "                rows_list.append(first_row)\n",
    "                rows_list.append(row)\n",
    "                curr_act = \"\"\n",
    "                \n",
    "    return pd.DataFrame(rows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_value_con(dep_1, dep_2, ea, df):\n",
    "    f1 = df.loc[df[activity] == dep_1][ea].to_frame().reset_index().drop(\"index\", axis=1)\n",
    "    f2 = df.loc[df[activity] == dep_2][ea].to_frame().reset_index().drop(\"index\", axis=1)\n",
    "    df_wo_na = pd.concat([f1,f2], axis= 1)\n",
    "    df_wo_na.columns = pd.RangeIndex(df_wo_na.columns.size)\n",
    "    df_wo_na = df_wo_na.dropna()\n",
    "    \n",
    "    l1 = list(df_wo_na[0])\n",
    "    l2 = list(df_wo_na[1])\n",
    "    df1 = df_wo_na[0]\n",
    "    df2 = df_wo_na[1]\n",
    "    \n",
    "    if((len(l1) < 8) | (len(l2) < 8)):\n",
    "        return(np.nan,np.nan, np.nan, np.nan,np.nan,np.nan, np.nan, np.nan)\n",
    "    try:\n",
    "        p = pg.wilcoxon(l1, l2)[\"p-val\"][0]\n",
    "        cles = pg.wilcoxon(l1, l2)[\"CLES\"][0]\n",
    "        rbc = pg.wilcoxon(l1, l2)[\"RBC\"][0]\n",
    "        z = stats.norm.isf(p / 2)\n",
    "        r = z / np.sqrt(len(l1)*2)        \n",
    "        cohen = 2*r / np.sqrt(1-np.square(r))\n",
    "        return (p, cles, rbc, len(l1), df1.mean(), df2.mean(), df1.std(), df2.std())\n",
    "    except:\n",
    "        return(1,0,0,0, 0, 0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\anaconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:3141: UserWarning: Exact p-value calculation does not work if there are ties. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "C:\\Users\\jonas\\anaconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:3155: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    }
   ],
   "source": [
    "con_All = pd.DataFrame()\n",
    "df_con = pd.DataFrame()\n",
    "for rel in dfg:\n",
    "    #varianten aus consecutive df extrahieren\n",
    "    consecutive_df = consecutive_hadms(final_pm_var, rel[0], rel[1])\n",
    "    variants = consecutive_df[\"variant\"].unique()\n",
    "    att_list = att_card.loc[att_card[activity].isin([rel[0], rel[1]])].sum().to_frame().reset_index()\n",
    "    att_list = att_list.rename({\"index\":\"e_At\", 0:\"cardinality\"}, axis=1)\n",
    "    att_list = att_list.loc[(att_list[\"cardinality\"] == 2) & (att_list[\"e_At\"].isin(attribute_list_con))].reset_index()\n",
    "    for e_at in att_list[\"e_At\"]:\n",
    "        p, cles, rbc, num_p, m1, m2, st1, st2 = stat_value_con(rel[0], rel[1], e_at, consecutive_df)\n",
    "        con_All = con_All.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"RBC\": rbc, 'abs(RBC)': abs(rbc), 'var' : 'ALL', '#Patients' : num_p, 'M1':m1, 'M2':m2, 'ST1':st1, 'ST2':st2, 'Directly':True}, ignore_index=True)    \n",
    "        if(p <= (0.05 / len(att_list))):\n",
    "            df_con = df_con.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"RBC\": rbc, 'abs(RBC)': abs(rbc), 'var' : 'ALL', '#Patients' : num_p, 'M1':m1, 'M2':m2, 'ST1':st1, 'ST2':st2, 'Directly':True}, ignore_index=True)\n",
    "        for var in variants:\n",
    "            df_var = consecutive_df.loc[consecutive_df[\"variant\"] == var]\n",
    "            p, cles, rbc, num_p, m1, m2, st1, st2 = stat_value_con(rel[0], rel[1], e_at, df_var)\n",
    "            con_All = con_All.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"RBC\": rbc, 'abs(RBC)': abs(rbc), 'var' : var, '#Patients' : num_p, 'M1':m1, 'M2':m2, 'ST1':st1, 'ST2':st2, 'Directly':True}, ignore_index=True)\n",
    "            if(p <= (0.05 / len(att_list))):\n",
    "                df_con = df_con.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"RBC\": rbc, 'abs(RBC)': abs(rbc), 'var' : var, '#Patients' : num_p, 'M1':m1, 'M2':m2, 'ST1':st1, 'ST2':st2, 'Directly':True}, ignore_index=True)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rel in efg_graph:\n",
    "    #varianten aus consecutive df extrahieren\n",
    "    consecutive_df = eventually_follow_hadms(final_pm_var, rel[0], rel[1])\n",
    "    variants = consecutive_df[\"variant\"].unique()\n",
    "    att_list = att_card.loc[att_card[activity].isin([rel[0], rel[1]])].sum().to_frame().reset_index()\n",
    "    att_list = att_list.rename({\"index\":\"e_At\", 0:\"cardinality\"}, axis=1)\n",
    "    att_list = att_list.loc[(att_list[\"cardinality\"] == 2) & (att_list[\"e_At\"].isin(attribute_list_con))].reset_index()\n",
    "    for e_at in att_list[\"e_At\"]:\n",
    "        p, cles, rbc, num_p, m1, m2, st1, st2 = stat_value_con(rel[0], rel[1], e_at, consecutive_df)\n",
    "        con_All = con_All.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"RBC\": rbc, 'abs(RBC)': abs(rbc), 'var' : 'ALL', '#Patients' : num_p, 'M1':m1, 'M2':m2, 'ST1':st1, 'ST2':st2, 'Directly':False}, ignore_index=True)    \n",
    "        if(p <= (0.05 / len(att_list))):\n",
    "            df_con = df_con.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"RBC\": rbc, 'abs(RBC)': abs(rbc), 'var' : 'ALL', '#Patients' : num_p, 'M1':m1, 'M2':m2, 'ST1':st1, 'ST2':st2, 'Directly':False}, ignore_index=True)\n",
    "            for var in variants:\n",
    "                df_var = consecutive_df.loc[consecutive_df[\"variant\"] == var]\n",
    "                p, cles, rbc, num_p, m1, m2, st1, st2 = stat_value_con(rel[0], rel[1], e_at, df_var)\n",
    "                con_All = con_All.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"RBC\": rbc, 'abs(RBC)': abs(rbc), 'var' : var, '#Patients' : num_p, 'M1':m1, 'M2':m2, 'ST1':st1, 'ST2':st2, 'Directly':False}, ignore_index=True)\n",
    "                if(p <= (0.05 / len(att_list))):\n",
    "                    df_con = df_con.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"RBC\": rbc, 'abs(RBC)': abs(rbc), 'var' : var, '#Patients' : num_p, 'M1':m1, 'M2':m2, 'ST1':st1, 'ST2':st2, 'Directly':False}, ignore_index=True)\n",
    "\n",
    "con_All = con_All.loc[~con_All[\"P\"].isna()]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Act_1</th>\n",
       "      <th>Act_2</th>\n",
       "      <th>E_At</th>\n",
       "      <th>P</th>\n",
       "      <th>RBC</th>\n",
       "      <th>abs(RBC)</th>\n",
       "      <th>var</th>\n",
       "      <th>#Patients</th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>ST1</th>\n",
       "      <th>ST2</th>\n",
       "      <th>Directly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pre-ICU Cardiology</td>\n",
       "      <td>Cardiac ICU</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>9.041414e-05</td>\n",
       "      <td>-0.483542</td>\n",
       "      <td>0.483542</td>\n",
       "      <td>ALL</td>\n",
       "      <td>88.0</td>\n",
       "      <td>130.333729</td>\n",
       "      <td>143.134063</td>\n",
       "      <td>42.432733</td>\n",
       "      <td>41.602721</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pre-ICU Cardiology</td>\n",
       "      <td>Cardiac ICU</td>\n",
       "      <td>Anion Gap</td>\n",
       "      <td>1.324767e-03</td>\n",
       "      <td>0.398824</td>\n",
       "      <td>0.398824</td>\n",
       "      <td>ALL</td>\n",
       "      <td>88.0</td>\n",
       "      <td>15.747746</td>\n",
       "      <td>14.994361</td>\n",
       "      <td>2.954144</td>\n",
       "      <td>3.121193</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pre-ICU Cardiology</td>\n",
       "      <td>Cardiac ICU</td>\n",
       "      <td>Calcium</td>\n",
       "      <td>4.598749e-05</td>\n",
       "      <td>0.521530</td>\n",
       "      <td>0.521530</td>\n",
       "      <td>ALL</td>\n",
       "      <td>82.0</td>\n",
       "      <td>8.865686</td>\n",
       "      <td>8.642665</td>\n",
       "      <td>0.484497</td>\n",
       "      <td>0.523748</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pre-ICU Cardiology</td>\n",
       "      <td>Cardiac ICU</td>\n",
       "      <td>Calcium</td>\n",
       "      <td>2.703464e-04</td>\n",
       "      <td>0.570370</td>\n",
       "      <td>0.570370</td>\n",
       "      <td>Emergency Department,Pre-ICU Cardiology,Cardia...</td>\n",
       "      <td>55.0</td>\n",
       "      <td>8.931007</td>\n",
       "      <td>8.670572</td>\n",
       "      <td>0.498668</td>\n",
       "      <td>0.540990</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pre-ICU Cardiology</td>\n",
       "      <td>Cardiac ICU</td>\n",
       "      <td>Hematocrit</td>\n",
       "      <td>9.851226e-08</td>\n",
       "      <td>0.669748</td>\n",
       "      <td>0.669748</td>\n",
       "      <td>ALL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>33.590186</td>\n",
       "      <td>31.743392</td>\n",
       "      <td>5.203964</td>\n",
       "      <td>4.931405</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>Pre-ICU Medicine</td>\n",
       "      <td>Surgical ICU</td>\n",
       "      <td>Red Blood Cells</td>\n",
       "      <td>7.538129e-07</td>\n",
       "      <td>0.653452</td>\n",
       "      <td>0.653452</td>\n",
       "      <td>ALL</td>\n",
       "      <td>76.0</td>\n",
       "      <td>3.495531</td>\n",
       "      <td>3.263263</td>\n",
       "      <td>0.791178</td>\n",
       "      <td>0.707653</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>Pre-ICU Medicine</td>\n",
       "      <td>Surgical ICU</td>\n",
       "      <td>Red Blood Cells</td>\n",
       "      <td>8.324187e-05</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>Emergency Department,Pre-ICU Medicine,Surgical...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>3.548521</td>\n",
       "      <td>3.321499</td>\n",
       "      <td>0.787344</td>\n",
       "      <td>0.697284</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>Pre-ICU Medicine</td>\n",
       "      <td>Medical ICU</td>\n",
       "      <td>Hematocrit</td>\n",
       "      <td>1.472802e-03</td>\n",
       "      <td>0.395082</td>\n",
       "      <td>0.395082</td>\n",
       "      <td>ALL</td>\n",
       "      <td>86.0</td>\n",
       "      <td>32.223473</td>\n",
       "      <td>31.059210</td>\n",
       "      <td>6.283784</td>\n",
       "      <td>5.620250</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>Pre-ICU Medicine</td>\n",
       "      <td>Medical ICU</td>\n",
       "      <td>Hematocrit</td>\n",
       "      <td>1.160171e-03</td>\n",
       "      <td>0.471230</td>\n",
       "      <td>0.471230</td>\n",
       "      <td>Emergency Department,Pre-ICU Medicine,Medical ...</td>\n",
       "      <td>63.0</td>\n",
       "      <td>32.125352</td>\n",
       "      <td>30.918262</td>\n",
       "      <td>6.499190</td>\n",
       "      <td>5.710478</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>Pre-ICU Medicine</td>\n",
       "      <td>Medical ICU</td>\n",
       "      <td>Red Blood Cells</td>\n",
       "      <td>1.722211e-03</td>\n",
       "      <td>0.398766</td>\n",
       "      <td>0.398766</td>\n",
       "      <td>ALL</td>\n",
       "      <td>83.0</td>\n",
       "      <td>3.674771</td>\n",
       "      <td>3.538095</td>\n",
       "      <td>0.786150</td>\n",
       "      <td>0.696807</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>962 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Act_1         Act_2             E_At             P  \\\n",
       "0    Pre-ICU Cardiology   Cardiac ICU          Glucose  9.041414e-05   \n",
       "1    Pre-ICU Cardiology   Cardiac ICU        Anion Gap  1.324767e-03   \n",
       "2    Pre-ICU Cardiology   Cardiac ICU          Calcium  4.598749e-05   \n",
       "3    Pre-ICU Cardiology   Cardiac ICU          Calcium  2.703464e-04   \n",
       "4    Pre-ICU Cardiology   Cardiac ICU       Hematocrit  9.851226e-08   \n",
       "..                  ...           ...              ...           ...   \n",
       "957    Pre-ICU Medicine  Surgical ICU  Red Blood Cells  7.538129e-07   \n",
       "958    Pre-ICU Medicine  Surgical ICU  Red Blood Cells  8.324187e-05   \n",
       "959    Pre-ICU Medicine   Medical ICU       Hematocrit  1.472802e-03   \n",
       "960    Pre-ICU Medicine   Medical ICU       Hematocrit  1.160171e-03   \n",
       "961    Pre-ICU Medicine   Medical ICU  Red Blood Cells  1.722211e-03   \n",
       "\n",
       "          RBC  abs(RBC)                                                var  \\\n",
       "0   -0.483542  0.483542                                                ALL   \n",
       "1    0.398824  0.398824                                                ALL   \n",
       "2    0.521530  0.521530                                                ALL   \n",
       "3    0.570370  0.570370  Emergency Department,Pre-ICU Cardiology,Cardia...   \n",
       "4    0.669748  0.669748                                                ALL   \n",
       "..        ...       ...                                                ...   \n",
       "957  0.653452  0.653452                                                ALL   \n",
       "958  0.616162  0.616162  Emergency Department,Pre-ICU Medicine,Surgical...   \n",
       "959  0.395082  0.395082                                                ALL   \n",
       "960  0.471230  0.471230  Emergency Department,Pre-ICU Medicine,Medical ...   \n",
       "961  0.398766  0.398766                                                ALL   \n",
       "\n",
       "     #Patients          M1          M2        ST1        ST2  Directly  \n",
       "0         88.0  130.333729  143.134063  42.432733  41.602721       1.0  \n",
       "1         88.0   15.747746   14.994361   2.954144   3.121193       1.0  \n",
       "2         82.0    8.865686    8.642665   0.484497   0.523748       1.0  \n",
       "3         55.0    8.931007    8.670572   0.498668   0.540990       1.0  \n",
       "4         84.0   33.590186   31.743392   5.203964   4.931405       1.0  \n",
       "..         ...         ...         ...        ...        ...       ...  \n",
       "957       76.0    3.495531    3.263263   0.791178   0.707653       0.0  \n",
       "958       54.0    3.548521    3.321499   0.787344   0.697284       0.0  \n",
       "959       86.0   32.223473   31.059210   6.283784   5.620250       0.0  \n",
       "960       63.0   32.125352   30.918262   6.499190   5.710478       0.0  \n",
       "961       83.0    3.674771    3.538095   0.786150   0.696807       0.0  \n",
       "\n",
       "[962 rows x 13 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stuart_maxwell(cons_df, dep1, dep2, att):\n",
    "    graph_stats = cons_df[[case_id, activity, att]]\n",
    "    to_remove = graph_stats.loc[graph_stats[att].isna()][case_id]\n",
    "    graph_stats = graph_stats.loc[~graph_stats[case_id].isin(to_remove)]\n",
    "    curr_hadm = \"\"\n",
    "    first_val = \"\"\n",
    "    second_val = \"\"\n",
    "    abnormal_col = graph_stats.columns[2]\n",
    "    val_count = graph_stats[abnormal_col].value_counts()\n",
    "    graph_cat = pd.DataFrame(columns=[\"Source\", \"Target\", \"Frequency\"])\n",
    "    for col_source in val_count.index:\n",
    "        for col_target in val_count.index:\n",
    "            new_row = {\"Source\":col_source, \"Target\":col_target, \"Frequency\": 0}\n",
    "            graph_cat = graph_cat.append(new_row, ignore_index=True)\n",
    "    for index, row in graph_stats.iterrows():\n",
    "        if(curr_hadm != row[case_id]):\n",
    "            curr_hadm = row[case_id]\n",
    "            first_val = row[abnormal_col]\n",
    "        else:\n",
    "            second_val = row[abnormal_col]\n",
    "            if((pd.isna(first_val)) | (pd.isna(second_val))):\n",
    "                pass\n",
    "            else:\n",
    "                freq = graph_cat.loc[(graph_cat[\"Source\"] == first_val) & (graph_cat[\"Target\"] == second_val)][\"Frequency\"].iloc[0]\n",
    "                graph_cat.loc[(graph_cat[\"Source\"] == first_val) & (graph_cat[\"Target\"] == second_val), \"Frequency\"] = freq+1\n",
    "    tab = graph_cat.set_index(['Source', 'Target'])\n",
    "    tab = tab.unstack()\n",
    "    tab.columns = tab.columns.get_level_values(1)\n",
    "    sqtab = ST(tab)\n",
    "    test = sqtab.homogeneity()\n",
    "    p = test.pvalue\n",
    "    chi2 = test.statistic\n",
    "    return tab, p, chi2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_value_cat(dep_1, dep_2, ea, df):\n",
    "    df_wo_na = df.loc[~df[ea].isna()]\n",
    "    summary = df_wo_na.groupby(case_id).count()\n",
    "    df_wo_na = summary.loc[(summary[activity] > 1) & (summary[activity] < 3)]\n",
    "    hadms_wo_na = list(df_wo_na.reset_index()[case_id])\n",
    "    df_wo_na = df.loc[df[case_id].isin(hadms_wo_na)]\n",
    "    df = df_wo_na\n",
    "    num_p = len(df.loc[(df[activity] == dep_1) & (~df[ea].isna())])\n",
    "    count_1 = df.loc[(df[activity] == dep_1) & (~df[ea].isna())][ea].value_counts()\n",
    "    count_2 = df.loc[(df[activity] == dep_2) & (~df[ea].isna())][ea].value_counts()\n",
    "    if((len(count_1) < 2) | (len(count_2) < 2)):\n",
    "        return(np.nan,np.nan, np.nan)\n",
    "    g, p, chi2 = stuart_maxwell(df, dep_1, dep_2, ea)\n",
    "    return (p, chi2, num_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_All = pd.DataFrame()\n",
    "df_cat = pd.DataFrame()\n",
    "for rel in dfg:\n",
    "    #varianten aus consecutive df extrahieren\n",
    "    consecutive_df = consecutive_hadms(final_pm_var, rel[0], rel[1])\n",
    "    variants = consecutive_df[\"variant\"].unique()\n",
    "    att_list = att_card.loc[att_card[activity].isin([rel[0], rel[1]])].sum().to_frame().reset_index()\n",
    "    att_list = att_list.rename({\"index\":\"e_At\", 0:\"cardinality\"}, axis=1)\n",
    "    att_list = att_list.loc[(att_list[\"cardinality\"] == 2) & (att_list[\"e_At\"].isin(attribute_list_cat))].reset_index()\n",
    "    for e_at in att_list[\"e_At\"]:\n",
    "        stat_value_cat(rel[0], rel[1], e_at, consecutive_df)\n",
    "        p, chi2, num_p = stat_value_cat(rel[0], rel[1], e_at, consecutive_df)\n",
    "        cat_All = cat_All.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"Chi2\": chi2, 'var' : 'ALL', '#Patients' : num_p, 'Directly':True}, ignore_index=True)\n",
    "        if(p <= (0.05) / len(att_list)):\n",
    "            df_cat = df_cat.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"Chi2\": chi2, 'var' : 'ALL', '#Patients' : num_p, 'Directly':True}, ignore_index=True)\n",
    "        for var in variants:\n",
    "            df_var = consecutive_df.loc[consecutive_df[\"variant\"] == var]\n",
    "            p, chi2, num_p = stat_value_cat(rel[0], rel[1], e_at, df_var)\n",
    "            cat_All = cat_All.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"Chi2\": chi2, 'var' : var, '#Patients' : num_p, 'Directly':True}, ignore_index=True)\n",
    "            if(p <= (0.05) / len(att_list)):\n",
    "                df_cat = df_cat.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"Chi2\": chi2, 'var' : var, '#Patients' : num_p, 'Directly':True}, ignore_index=True)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rel in efg_graph:\n",
    "    #varianten aus consecutive df extrahieren\n",
    "    consecutive_df = eventually_follow_hadms(final_pm_var, rel[0], rel[1])\n",
    "    variants = consecutive_df[\"variant\"].unique()\n",
    "    att_list = att_card.loc[att_card[activity].isin([rel[0], rel[1]])].sum().to_frame().reset_index()\n",
    "    att_list = att_list.rename({\"index\":\"e_At\", 0:\"cardinality\"}, axis=1)\n",
    "    att_list = att_list.loc[(att_list[\"cardinality\"] == 2) & (att_list[\"e_At\"].isin(attribute_list_cat))].reset_index()\n",
    "    for e_at in att_list[\"e_At\"]:\n",
    "        stat_value_cat(rel[0], rel[1], e_at, consecutive_df)\n",
    "        p, chi2, num_p = stat_value_cat(rel[0], rel[1], e_at, consecutive_df)\n",
    "        cat_All = cat_All.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"Chi2\": chi2, 'var' : 'ALL', '#Patients' : num_p, 'Directly':False}, ignore_index=True)\n",
    "        if(p <= (0.05) / len(att_list)):\n",
    "            df_cat = df_cat.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"Chi2\": chi2, 'var' : 'ALL', '#Patients' : num_p, 'Directly':False}, ignore_index=True)\n",
    "        for var in variants:\n",
    "            df_var = consecutive_df.loc[consecutive_df[\"variant\"] == var]\n",
    "            p, chi2, num_p = stat_value_cat(rel[0], rel[1], e_at, df_var)\n",
    "            cat_All = cat_All.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"Chi2\": chi2, 'var' : var, '#Patients' : num_p, 'Directly':False}, ignore_index=True)\n",
    "            if(p <= (0.05)/ len(att_list)):\n",
    "                df_cat = df_cat.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"Chi2\": chi2, 'var' : var, '#Patients' : num_p, 'Directly':False}, ignore_index=True)\n",
    "cat_All = cat_All.loc[~cat_All[\"P\"].isna()]         \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MIMIC\n",
    "con_All.to_csv(\"Outputs/Change_Detection_Kidney_con_All.csv\")\n",
    "df_con.to_csv(\"Outputs/Change_Detection_Kidney_df_con.csv\")\n",
    "cat_All.to_csv(\"Outputs/Change_Detection_Kidney_cat_All.csv\")\n",
    "df_cat[\"Con_E_At\"] = df_cat[\"E_At\"].str.split(' ', 1, expand=True)[1]\n",
    "df_cat.to_csv(\"Outputs/Change_Detection_Kidney_df_cat.csv\")\n",
    "final_pm_var.to_csv(\"Outputs/Kidney_Transformed_Var.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
